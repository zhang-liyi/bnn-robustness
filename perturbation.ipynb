{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUkBNeEeNxcZ"
   },
   "source": [
    "# Deep Learning Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vi-P-cgcPdr_"
   },
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdXIzBqcEnGQ",
    "outputId": "05f4c82c-de74-4d94-d048-10d5bd481fa5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.nn import softmax\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "x_train, x_val = tf.split(x_train, [50000, 10000], axis=0)\n",
    "y_train, y_val = tf.split(y_train, [50000, 10000], axis=0)\n",
    "\n",
    "batch_size = 10000\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "val_size = x_val.shape[0]\n",
    "img_length = x_train.shape[1] * x_train.shape[2]\n",
    "img_shape1 = x_train.shape[1]\n",
    "img_shape2 = x_train.shape[2]\n",
    "num_classes = 10\n",
    "\n",
    "x_train = x_train/126\n",
    "x_val = x_val/126\n",
    "x_test = x_test/126\n",
    "y_train = tf.one_hot(y_train, num_classes)/1\n",
    "y_val = tf.one_hot(y_val, num_classes)/1\n",
    "y_test = tf.one_hot(y_test, num_classes)/1\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
    "#val_dataset = val_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "path = 'results/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e59qJjNAPgE3"
   },
   "source": [
    "Define, train, and evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FcT15jGEnJl"
   },
   "outputs": [],
   "source": [
    "x_in = Input(shape=(x_train.shape[1],x_train.shape[2]))\n",
    "x = Flatten()(x_in)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(x_in, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Amar6u-mEnL5"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=CategoricalCrossentropy(from_logits=False), optimizer=SGD(lr=1e-3, momentum=0.95), metrics=CategoricalAccuracy())\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6riUtq_EnPG",
    "outputId": "99ba9266-6882-4b85-d712-5b38979cee5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1013 - categorical_accuracy: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10134784132242203, 0.9786999821662903]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5kPuNWeIwEO"
   },
   "outputs": [],
   "source": [
    "# model.save(path+'model_xxx/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxfHxe4nI7mY"
   },
   "outputs": [],
   "source": [
    "# model = load_model(path+'model_1200_1200_10_relu/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvVw44oiN4FC"
   },
   "source": [
    "# Data Perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExY8kMYKaF3B"
   },
   "source": [
    "**Main section for performing data perturbation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "NgyKKf7tK6AV"
   },
   "outputs": [],
   "source": [
    "def objective(r, model, x, y, c=0.5, bayesian=False):\n",
    "    norm = tf.reduce_mean(tf.norm(r, axis=(1,2)))\n",
    "    perturbed_img = x + r\n",
    "    y_pred = model(perturbed_img)\n",
    "    loss_value = 1/c*CategoricalCrossentropy()(y, y_pred)\n",
    "\n",
    "    return (norm + loss_value)\n",
    "\n",
    "def perturb_data(model):\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    r_matrices = np.zeros((val_size, num_classes, img_shape1, img_shape2))\n",
    "    r_norms = np.zeros((val_size, num_classes))\n",
    "\n",
    "    for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
    "        for k in range(num_classes):\n",
    "            print('Class ' + str(k))\n",
    "            \n",
    "            r = tf.Variable(np.zeros((x_batch_val.shape[0], img_shape1, img_shape2))+2e-2, \n",
    "                            constraint = lambda x: tf.clip_by_value(x, 1e-6, 2.03 - x_batch_val),\n",
    "                            dtype=tf.float32, name='r')\n",
    "            y_fake = np.zeros((x_batch_val.shape[0], num_classes))\n",
    "            y_fake[:, k] = 1.\n",
    "        \n",
    "            epochs = 4000\n",
    "            for epoch in range(epochs):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss_value = objective(r, model, x_batch_val, y_fake, c=2*(epoch+1)/epochs)\n",
    "                    grads = tape.gradient(loss_value, [r])\n",
    "                optimizer.apply_gradients(zip(grads, [r]))\n",
    "\n",
    "                # y_pred = softmax(model(x_batch_val+r), axis=1)\n",
    "                # y_pred = y_pred.numpy()\n",
    "\n",
    "                if epoch % 100 == 0:\n",
    "                    print('Epoch ' + str(epoch))\n",
    "                    # count = 0\n",
    "                    # norm_count = 0\n",
    "                    # for i in range(x_batch_val.shape[0]):\n",
    "                    #     if np.argmax(y_pred[i,:]) == k and np.argmax(y_batch_val[i,:]) != k:\n",
    "                    #         count += 1\n",
    "                    #         norm_count += tf.norm(r[i]).numpy()\n",
    "                    # print(' -- Finding rate: ' + str(count/x_batch_val.shape[0]))\n",
    "                    # print(' -- Norm of r: ' + str(norm_count/x_batch_val.shape[0]))\n",
    "            \n",
    "            y_pred = model(x_batch_val+r).numpy()\n",
    "            \n",
    "            for j in range(x_batch_val.shape[0]):\n",
    "                i = int(step * batch_size + j)\n",
    "                if np.argmax(y_pred[i,:]) == k and np.argmax(y_batch_val[i,:]) != k:\n",
    "                    r_norms[i,k] = tf.norm(r[i]).numpy()\n",
    "                    r_matrices[i,k,:,:] = r[i].numpy()\n",
    "                else:\n",
    "                    r_norms[i,k] = 1e6\n",
    "                # print(r_norms[i,:])\n",
    "    return r_norms, r_matrices\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpddLCAEnro3"
   },
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "jmten7-BTdTW"
   },
   "outputs": [],
   "source": [
    "model_names = ['model_10','model_10_l2_3','model_10_l2_2','model_10_l2_1',\n",
    "               'model_100_100_10_relu','model_100_100_10_relu_l2_3','model_100_100_10_relu_l2_2','model_100_100_10_relu_l2_1',\n",
    "               'model_200_200_10_relu','model_200_200_10_relu_l2_3','model_200_200_10_relu_l2_2','model_200_200_10_relu_l2_1',\n",
    "               'model_1200_1200_10_relu','model_1200_1200_10_relu_l2_3','model_1200_1200_10_relu_l2_2','model_1200_1200_10_relu_l2_1']\n",
    "\n",
    "bayes_names = ['bayes_30_0','bayes_10_0','bayes_4_0','bayes_3_0','bayes_2_5','bayes_2_0','bayes_1_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lS2CNWqDkowG"
   },
   "outputs": [],
   "source": [
    "for i in range(len(model_names)):\n",
    "    name = model_names[i]\n",
    "    model = load_model(path+'models/'+name+'/')\n",
    "    r_norms, r_matrices = perturb_data(model)\n",
    "    with open(path+'r_matrices/r_matrices_' + name + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(r_matrices, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBkD1Nk1nuCO"
   },
   "source": [
    "Get saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "QngHSjfInLEA",
    "outputId": "16026272-d678-4542-f233-ae8ed99c9a1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "r_matrices_16 = np.zeros((16, val_size, num_classes, img_shape1, img_shape2))\n",
    "models = []\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    name = model_names[i]\n",
    "    models.append(load_model(path + 'models/' + name + '/'))\n",
    "    r_matrices_16[i] = pd.read_pickle(path+'r_matrices/r_matrices_' + name + '.pickle')\n",
    "    print(i)\n",
    "\n",
    "with open(path+'r_matrices/r_matrices_16.pickle', 'wb') as handle:\n",
    "    pickle.dump(r_matrices_16, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "bayes_models = []\n",
    "\n",
    "for i in range(len(bayes_names)):\n",
    "    name = bayes_names[i]\n",
    "    bayes_models.append(load_model(path + 'models/' + name + '/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "FOUScQimbCJy"
   },
   "outputs": [],
   "source": [
    "r_norms = np.zeros((16, val_size, 10))\n",
    "for m in range(16):\n",
    "    for i in range(val_size):\n",
    "        for k in range(10):\n",
    "            r_norms[m,i,k] = np.linalg.norm(r_matrices_16[m,i,k,:,:])\n",
    "            if r_norms[m,i,k] == 0:\n",
    "                r_norms[m,i,k] = 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAzjFnZaZRjE"
   },
   "source": [
    "Below is how a model would classify a given example, and how it would classify the perturbed example. The example and its perturbed version are also illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+06 1.00000000e+06 1.00000000e+06 1.00000000e+06\n",
      " 1.95041215e-02 1.00000000e+06 1.00000000e+06 1.00000000e+06\n",
      " 1.00000000e+06 1.00000000e+06]\n"
     ]
    }
   ],
   "source": [
    "print(r_norms[15,91,:])\n",
    "model = models[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8vxhpvvfwHt",
    "outputId": "32e5f6b6-3edb-4ad9-8a10-6680eff8c578"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[0.00347512, 0.00357238, 0.01650848, 0.00187461, 0.54375637,\n",
       "        0.07611299, 0.07216172, 0.0035009 , 0.26875702, 0.01028045]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tf.expand_dims(x_val[91], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G7er_360fzL4",
    "outputId": "797c7235-66b9-4803-b271-95e6b66beaa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[0.00345568, 0.00355251, 0.0164209 , 0.00186601, 0.54737425,\n",
       "        0.07519767, 0.07151583, 0.00350654, 0.2667984 , 0.01031214]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tf.expand_dims(x_val[91]+r_matrices_16[15,91,4,:,:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "MF1r4evGf3LL",
    "outputId": "6a3a788c-4845-4f24-b66b-8b48a9388bd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa2d304df50>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_val[91])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "DKw5knZAPkM6",
    "outputId": "560f07b0-54a0-4f07-c46e-936fd2fb1ded"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa2cd0a24d0>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_val[91]+r_matrices_16[15,91,4,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "9wAgb-vchNIy"
   },
   "outputs": [],
   "source": [
    "def get_perturbations(m, multiplier=1):\n",
    "    y_perturbed = y_val.numpy()\n",
    "    x_perturbed = x_val.numpy()\n",
    "    remove_indices = []\n",
    "    distortion = 0\n",
    "    for i in range(val_size):\n",
    "        if sum(r_norms[m,i,:]) >= 1e7:\n",
    "            remove_indices.append(i)\n",
    "        else:\n",
    "            k = np.argmin(r_norms[m,i,:])\n",
    "            x_perturbed[i] = x_perturbed[i] + r_matrices_16[m,i,k,:,:]*multiplier\n",
    "            distortion += (np.sum(np.multiply(r_matrices_16[m,i,k,:,:], r_matrices_16[m,i,k,:,:]))/784)**0.5\n",
    "            \n",
    "    x_perturbed = np.delete(x_perturbed, remove_indices, axis=0)\n",
    "    y_perturbed = np.delete(y_perturbed, remove_indices, axis=0)\n",
    "    distortion = distortion / x_perturbed.shape[0]\n",
    "    return x_perturbed, y_perturbed, distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "id": "MSFSc1gR1pA_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_10\n",
      "(8351, 28, 28)\n",
      "0.0593779909460685\n",
      "---------------\n",
      "model_10_l2_3\n",
      "(7710, 28, 28)\n",
      "0.062148993435847295\n",
      "---------------\n",
      "model_10_l2_2\n",
      "(772, 28, 28)\n",
      "0.001316872849489344\n",
      "---------------\n",
      "model_10_l2_1\n",
      "(1117, 28, 28)\n",
      "0.0003831459854750663\n",
      "---------------\n",
      "model_100_100_10_relu\n",
      "(6895, 28, 28)\n",
      "0.05034573498653662\n",
      "---------------\n",
      "model_100_100_10_relu_l2_3\n",
      "(6748, 28, 28)\n",
      "0.04869280265941889\n",
      "---------------\n",
      "model_100_100_10_relu_l2_2\n",
      "(8929, 28, 28)\n",
      "0.07108349273440065\n",
      "---------------\n",
      "model_100_100_10_relu_l2_1\n",
      "(863, 28, 28)\n",
      "0.0005807538911293781\n",
      "---------------\n",
      "model_200_200_10_relu\n",
      "(7315, 28, 28)\n",
      "0.05568597814738444\n",
      "---------------\n",
      "model_200_200_10_relu_l2_3\n",
      "(7601, 28, 28)\n",
      "0.05613104089074708\n",
      "---------------\n",
      "model_200_200_10_relu_l2_2\n",
      "(8721, 28, 28)\n",
      "0.07419748740833922\n",
      "---------------\n",
      "model_200_200_10_relu_l2_1\n",
      "(669, 28, 28)\n",
      "0.0006460623718374734\n",
      "---------------\n",
      "model_1200_1200_10_relu\n",
      "(8248, 28, 28)\n",
      "0.06935490155309204\n",
      "---------------\n",
      "model_1200_1200_10_relu_l2_3\n",
      "(8248, 28, 28)\n",
      "0.0708185041912135\n",
      "---------------\n",
      "model_1200_1200_10_relu_l2_2\n",
      "(7867, 28, 28)\n",
      "0.08349892709064759\n",
      "---------------\n",
      "model_1200_1200_10_relu_l2_1\n",
      "(339, 28, 28)\n",
      "0.0008533637641142125\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "distortions = []\n",
    "for m in range(16):\n",
    "    x_perturbed, y_perturbed, distortion = get_perturbations(m)\n",
    "    distortions.append(distortion)\n",
    "    print(model_names[m])\n",
    "    print(x_perturbed.shape)\n",
    "    print(distortion)\n",
    "    print('---------------')\n",
    "np.savetxt('distortions.csv', np.array(distortions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for running Bayesian NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_BAYESIAN = load_model(path+'models/bayes_3_0/')\n",
    "'''\n",
    "When calling these functions, need to define global variable MODEL_BAYESIAN\n",
    "'''\n",
    "\n",
    "def call_model_bayesian(x, eval_samples=1, num_classes=num_classes):\n",
    "    pred_list = tf.constant(tf.zeros((1, x.shape[0], num_classes)))\n",
    "    pred_list, x_, s_, i_ = tf.while_loop(cond_eval_samples,\n",
    "                                        body_eval_samples, \n",
    "                                        loop_vars=[pred_list, x, \n",
    "                                                    eval_samples, 0],\n",
    "                                        shape_invariants=[tf.TensorShape([None, x.shape[0], num_classes]), \n",
    "                                                            x.shape,\n",
    "                                                            tf.TensorShape([]),\n",
    "                                                            tf.TensorShape([])])\n",
    "    pred_list = tf.gather(pred_list, list(range(1, eval_samples+1)))\n",
    "    pred_value = tf.reduce_mean(pred_list, axis=0)\n",
    "\n",
    "    return pred_value\n",
    "\n",
    "def evaluate_bayesian(x, y, eval_samples=1):\n",
    "    # 100/eval_samples had better be an integer\n",
    "    \n",
    "    metric = CategoricalAccuracy()\n",
    "    \n",
    "    acc_list = []\n",
    "    for s in range(int(100/eval_samples)):\n",
    "        pred_value = call_model_bayesian(x, eval_samples)\n",
    "        metric.update_state(y, pred_value)\n",
    "        acc = metric.result()\n",
    "        metric.reset_states()\n",
    "        acc_list.append(float(acc))\n",
    "        \n",
    "    accuracy = np.mean(acc_list)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def body_eval_samples(pred_list, x, eval_samples, i):\n",
    "    logits = MODEL_BAYESIAN(x)\n",
    "    pred_value = softmax(logits)\n",
    "    pred_list = tf.concat([pred_list, [pred_value]], axis=0)\n",
    "    i = i + 1\n",
    "    return pred_list, x, eval_samples, i\n",
    "\n",
    "def cond_eval_samples(pred_list, x, eval_samples, i):\n",
    "    return i < eval_samples\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "_xRUoSSeUimG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_10\n",
      "313/313 [==============================] - 0s 865us/step - loss: 0.2585 - categorical_accuracy: 0.9310\n",
      "model_10_l2_3\n",
      "313/313 [==============================] - 0s 713us/step - loss: 0.2607 - categorical_accuracy: 0.9298\n",
      "model_10_l2_2\n",
      "313/313 [==============================] - 0s 861us/step - loss: 0.3084 - categorical_accuracy: 0.9267\n",
      "model_10_l2_1\n",
      "313/313 [==============================] - 0s 993us/step - loss: 0.8538 - categorical_accuracy: 0.8891\n",
      "model_100_100_10_relu\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0969 - categorical_accuracy: 0.9757\n",
      "model_100_100_10_relu_l2_3\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1067 - categorical_accuracy: 0.9762\n",
      "model_100_100_10_relu_l2_2\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1279 - categorical_accuracy: 0.9780\n",
      "model_100_100_10_relu_l2_1\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.8775 - categorical_accuracy: 0.9144\n",
      "model_200_200_10_relu\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0925 - categorical_accuracy: 0.9791\n",
      "model_200_200_10_relu_l2_3\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0859 - categorical_accuracy: 0.9785\n",
      "model_200_200_10_relu_l2_2\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.1136 - categorical_accuracy: 0.9810\n",
      "model_200_200_10_relu_l2_1\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6752 - categorical_accuracy: 0.9337\n",
      "model_1200_1200_10_relu\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0738 - categorical_accuracy: 0.9811\n",
      "model_1200_1200_10_relu_l2_3\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0753 - categorical_accuracy: 0.9809\n",
      "model_1200_1200_10_relu_l2_2\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0992 - categorical_accuracy: 0.9820\n",
      "model_1200_1200_10_relu_l2_1\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.3891 - categorical_accuracy: 0.9671\n",
      "Bayesian NN validation accuracy: 0.9751\n",
      "Bayesian NN validation accuracy: 0.9728\n",
      "Bayesian NN validation accuracy: 0.9743\n",
      "Bayesian NN validation accuracy: 0.9724\n",
      "Bayesian NN validation accuracy: 0.9673\n",
      "Bayesian NN validation accuracy: 0.9622\n",
      "Bayesian NN validation accuracy: 0.9505\n"
     ]
    }
   ],
   "source": [
    "val_acc = []\n",
    "for m in range(16):\n",
    "    print(model_names[m])\n",
    "    lst = models[m].evaluate(x_val, y_val)\n",
    "    val_acc.append(lst[1])\n",
    "for m in range(len(bayes_models)):\n",
    "    MODEL_BAYESIAN = bayes_models[m]\n",
    "    acc = evaluate_bayesian(x_val, y_val)\n",
    "    val_acc.append(acc)\n",
    "    print(\"Bayesian NN validation accuracy: %.4f\" % (acc,))\n",
    "    \n",
    "np.savetxt('val_acc.csv', np.array(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_10\n",
      "313/313 [==============================] - 0s 904us/step - loss: 0.2670 - categorical_accuracy: 0.9261\n",
      "model_10_l2_3\n",
      "313/313 [==============================] - 0s 788us/step - loss: 0.2685 - categorical_accuracy: 0.9258\n",
      "model_10_l2_2\n",
      "313/313 [==============================] - 0s 791us/step - loss: 0.3165 - categorical_accuracy: 0.9234\n",
      "model_10_l2_1\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.8650 - categorical_accuracy: 0.8875\n",
      "model_100_100_10_relu\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0954 - categorical_accuracy: 0.9757\n",
      "model_100_100_10_relu_l2_3\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0994 - categorical_accuracy: 0.9764\n",
      "model_100_100_10_relu_l2_2\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1196 - categorical_accuracy: 0.9794\n",
      "model_100_100_10_relu_l2_1\n",
      "313/313 [==============================] - 0s 951us/step - loss: 0.8847 - categorical_accuracy: 0.9091\n",
      "model_200_200_10_relu\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0849 - categorical_accuracy: 0.9779\n",
      "model_200_200_10_relu_l2_3\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0828 - categorical_accuracy: 0.9788\n",
      "model_200_200_10_relu_l2_2\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1069 - categorical_accuracy: 0.9796\n",
      "model_200_200_10_relu_l2_1\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6833 - categorical_accuracy: 0.9295: 0s - loss: 0.7115 - cate\n",
      "model_1200_1200_10_relu\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0658 - categorical_accuracy: 0.9809\n",
      "model_1200_1200_10_relu_l2_3\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0645 - categorical_accuracy: 0.9815\n",
      "model_1200_1200_10_relu_l2_2\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0933 - categorical_accuracy: 0.9823\n",
      "model_1200_1200_10_relu_l2_1\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3907 - categorical_accuracy: 0.9640\n",
      "Bayesian NN test accuracy: 0.9751\n",
      "Bayesian NN test accuracy: 0.9746\n",
      "Bayesian NN test accuracy: 0.9737\n",
      "Bayesian NN test accuracy: 0.9712\n",
      "Bayesian NN test accuracy: 0.9661\n",
      "Bayesian NN test accuracy: 0.9611\n",
      "Bayesian NN test accuracy: 0.9459\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "for m in range(16):\n",
    "    print(model_names[m])\n",
    "    lst = models[m].evaluate(x_test, y_test)\n",
    "    test_acc.append(lst[1])\n",
    "for m in range(len(bayes_models)):\n",
    "    MODEL_BAYESIAN = bayes_models[m]\n",
    "    acc = evaluate_bayesian(x_test, y_test)\n",
    "    test_acc.append(acc)\n",
    "    print(\"Bayesian NN test accuracy: %.4f\" % (acc,))\n",
    "    \n",
    "np.savetxt('test_acc.csv', np.array(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on Perturbed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "id": "i-HK61X5YzeC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_10\n",
      "261/261 [==============================] - 0s 750us/step - loss: 14.5458 - categorical_accuracy: 0.0000e+00\n",
      "261/261 [==============================] - 0s 947us/step - loss: 12.3387 - categorical_accuracy: 0.0000e+00\n",
      "261/261 [==============================] - 0s 1ms/step - loss: 4.6324 - categorical_accuracy: 0.0115\n",
      "261/261 [==============================] - 0s 827us/step - loss: 1.1739 - categorical_accuracy: 0.8025\n",
      "261/261 [==============================] - 1s 3ms/step - loss: 3.3140 - categorical_accuracy: 0.4568\n",
      "261/261 [==============================] - 2s 6ms/step - loss: 2.9384 - categorical_accuracy: 0.4774\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 2.4758 - categorical_accuracy: 0.4858\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 1.0751 - categorical_accuracy: 0.7000\n",
      "261/261 [==============================] - 0s 1ms/step - loss: 3.4353 - categorical_accuracy: 0.4895\n",
      "261/261 [==============================] - 0s 1ms/step - loss: 3.7212 - categorical_accuracy: 0.4487\n",
      "261/261 [==============================] - 0s 1ms/step - loss: 2.4911 - categorical_accuracy: 0.5072\n",
      "261/261 [==============================] - 0s 1ms/step - loss: 1.1333 - categorical_accuracy: 0.7869\n",
      "261/261 [==============================] - 0s 1ms/step - loss: 4.1683 - categorical_accuracy: 0.4677\n",
      "261/261 [==============================] - 0s 1ms/step - loss: 5.5101 - categorical_accuracy: 0.3748\n",
      "261/261 [==============================] - 0s 1ms/step - loss: 3.1435 - categorical_accuracy: 0.4183\n",
      "261/261 [==============================] - 0s 1ms/step - loss: 1.3326 - categorical_accuracy: 0.7682\n",
      "model_10_l2_3\n",
      "241/241 [==============================] - 0s 782us/step - loss: 14.2237 - categorical_accuracy: 1.2970e-04\n",
      "241/241 [==============================] - 0s 797us/step - loss: 14.6473 - categorical_accuracy: 0.0000e+00\n",
      "241/241 [==============================] - 0s 785us/step - loss: 5.1973 - categorical_accuracy: 0.0109\n",
      "241/241 [==============================] - 0s 800us/step - loss: 1.2108 - categorical_accuracy: 0.7878\n",
      "241/241 [==============================] - 1s 4ms/step - loss: 3.8589 - categorical_accuracy: 0.4191\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 3.4892 - categorical_accuracy: 0.4311\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 2.9597 - categorical_accuracy: 0.4367\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 1.2541 - categorical_accuracy: 0.6358\n",
      "241/241 [==============================] - 0s 1ms/step - loss: 4.2366 - categorical_accuracy: 0.4314\n",
      "241/241 [==============================] - 0s 1ms/step - loss: 4.4355 - categorical_accuracy: 0.4048\n",
      "241/241 [==============================] - 0s 1ms/step - loss: 2.8855 - categorical_accuracy: 0.4681\n",
      "241/241 [==============================] - 1s 3ms/step - loss: 1.2261 - categorical_accuracy: 0.7482\n",
      "241/241 [==============================] - 0s 1ms/step - loss: 4.7259 - categorical_accuracy: 0.4306\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 6.3840 - categorical_accuracy: 0.3375\n",
      "241/241 [==============================] - 0s 1ms/step - loss: 3.6747 - categorical_accuracy: 0.3754\n",
      "241/241 [==============================] - 0s 1ms/step - loss: 1.4205 - categorical_accuracy: 0.7274\n",
      "model_10_l2_2\n",
      "25/25 [==============================] - 0s 817us/step - loss: 2.6115 - categorical_accuracy: 0.1088\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.6064 - categorical_accuracy: 0.0972\n",
      "25/25 [==============================] - 0s 798us/step - loss: 2.4359 - categorical_accuracy: 0.0000e+00\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.2401 - categorical_accuracy: 0.1464\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8143 - categorical_accuracy: 0.7979\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.8431 - categorical_accuracy: 0.7940\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.7673 - categorical_accuracy: 0.8031\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.3869 - categorical_accuracy: 0.5894\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.0082 - categorical_accuracy: 0.7953\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.9040 - categorical_accuracy: 0.7863\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8005 - categorical_accuracy: 0.8005\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.0790 - categorical_accuracy: 0.3070\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.9971 - categorical_accuracy: 0.7733\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1313 - categorical_accuracy: 0.7668\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.8934 - categorical_accuracy: 0.7785\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.3957 - categorical_accuracy: 0.2137\n",
      "model_10_l2_1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 1.7146 - categorical_accuracy: 0.4996\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 1.7213 - categorical_accuracy: 0.4888\n",
      "35/35 [==============================] - 0s 905us/step - loss: 1.7278 - categorical_accuracy: 0.4306\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 2.1489 - categorical_accuracy: 0.0000e+00\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5565 - categorical_accuracy: 0.8657\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5767 - categorical_accuracy: 0.8630\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5289 - categorical_accuracy: 0.8684\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 1.0830 - categorical_accuracy: 0.7278\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.6901 - categorical_accuracy: 0.8559\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 0.6150 - categorical_accuracy: 0.8532\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.5674 - categorical_accuracy: 0.8612\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 1.7759 - categorical_accuracy: 0.4727\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 0.6871 - categorical_accuracy: 0.8451\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.7667 - categorical_accuracy: 0.8451\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.6284 - categorical_accuracy: 0.8496\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 2.1308 - categorical_accuracy: 0.3384\n",
      "model_1200_1200_10_relu\n",
      "258/258 [==============================] - 0s 774us/step - loss: 7.1290 - categorical_accuracy: 0.0234\n",
      "258/258 [==============================] - 0s 888us/step - loss: 6.7014 - categorical_accuracy: 0.0268\n",
      "258/258 [==============================] - 0s 885us/step - loss: 2.8301 - categorical_accuracy: 0.1215\n",
      "258/258 [==============================] - 0s 878us/step - loss: 1.2981 - categorical_accuracy: 0.6883\n",
      "258/258 [==============================] - 1s 4ms/step - loss: 21.1390 - categorical_accuracy: 0.0000e+00\n",
      "258/258 [==============================] - 1s 5ms/step - loss: 8.7043 - categorical_accuracy: 0.0145\n",
      "258/258 [==============================] - 1s 5ms/step - loss: 7.2892 - categorical_accuracy: 0.0164\n",
      "258/258 [==============================] - 1s 6ms/step - loss: 1.7197 - categorical_accuracy: 0.3278\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 11.2350 - categorical_accuracy: 0.0195\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 11.1008 - categorical_accuracy: 0.0226\n",
      "258/258 [==============================] - 1s 2ms/step - loss: 7.8669 - categorical_accuracy: 0.0241: 0s - loss: 7.8056 - categorical_accu\n",
      "258/258 [==============================] - 1s 2ms/step - loss: 1.3687 - categorical_accuracy: 0.6000\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 12.2822 - categorical_accuracy: 0.0246\n",
      "258/258 [==============================] - 1s 2ms/step - loss: 13.5032 - categorical_accuracy: 0.0227 ETA: 0s - loss: 13.3542 - categorical_accuracy\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 8.2862 - categorical_accuracy: 0.0247\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 1.4860 - categorical_accuracy: 0.6216\n",
      "model_1200_1200_10_relu_l2_3\n",
      "258/258 [==============================] - 0s 767us/step - loss: 7.2237 - categorical_accuracy: 0.0241\n",
      "258/258 [==============================] - 0s 752us/step - loss: 6.7309 - categorical_accuracy: 0.0264\n",
      "258/258 [==============================] - 0s 858us/step - loss: 2.8860 - categorical_accuracy: 0.1199\n",
      "258/258 [==============================] - 0s 759us/step - loss: 1.3107 - categorical_accuracy: 0.6845\n",
      "258/258 [==============================] - 1s 3ms/step - loss: 9.4310 - categorical_accuracy: 0.0102\n",
      "258/258 [==============================] - 1s 5ms/step - loss: 21.1786 - categorical_accuracy: 0.0000e+00\n",
      "258/258 [==============================] - 1s 5ms/step - loss: 7.6947 - categorical_accuracy: 0.0115\n",
      "258/258 [==============================] - 1s 5ms/step - loss: 1.7649 - categorical_accuracy: 0.3082\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 11.5706 - categorical_accuracy: 0.0171\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 11.4862 - categorical_accuracy: 0.0179\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 8.0734 - categorical_accuracy: 0.0228\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 1.3929 - categorical_accuracy: 0.5820\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 12.5557 - categorical_accuracy: 0.0204\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 14.1166 - categorical_accuracy: 0.0198\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 8.5268 - categorical_accuracy: 0.0226\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 1.5052 - categorical_accuracy: 0.6130\n",
      "model_1200_1200_10_relu_l2_2\n",
      "246/246 [==============================] - 0s 771us/step - loss: 10.3368 - categorical_accuracy: 0.0107\n",
      "246/246 [==============================] - 0s 735us/step - loss: 9.7230 - categorical_accuracy: 0.0114\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 4.3083 - categorical_accuracy: 0.0470: 0s - loss: 4.3474 - categorical_accuracy\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 1.4597 - categorical_accuracy: 0.5923\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 14.9974 - categorical_accuracy: 0.0053\n",
      "246/246 [==============================] - 2s 7ms/step - loss: 14.9037 - categorical_accuracy: 0.0046\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 19.9194 - categorical_accuracy: 0.0000e+00\n",
      "246/246 [==============================] - 2s 7ms/step - loss: 3.0321 - categorical_accuracy: 0.0803\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 18.5105 - categorical_accuracy: 0.0080\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 18.0834 - categorical_accuracy: 0.0075\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 13.0584 - categorical_accuracy: 0.0083\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 1.8505 - categorical_accuracy: 0.3707\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 20.1731 - categorical_accuracy: 0.0086\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 22.0900 - categorical_accuracy: 0.0078\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 13.8555 - categorical_accuracy: 0.0086\n",
      "246/246 [==============================] - 0s 2ms/step - loss: 1.8524 - categorical_accuracy: 0.4286\n",
      "model_1200_1200_10_relu_l2_1\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6653 - categorical_accuracy: 0.1239\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 3.6673 - categorical_accuracy: 0.1121\n",
      "11/11 [==============================] - 0s 938us/step - loss: 3.3106 - categorical_accuracy: 0.0885\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5937 - categorical_accuracy: 0.0914\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.8613 - categorical_accuracy: 0.5398\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9088 - categorical_accuracy: 0.5487\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.6585 - categorical_accuracy: 0.5634\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3684 - categorical_accuracy: 0.0000e+00\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2801 - categorical_accuracy: 0.5339\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0365 - categorical_accuracy: 0.5428\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7480 - categorical_accuracy: 0.5428\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8442 - categorical_accuracy: 0.0796\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2488 - categorical_accuracy: 0.5192\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5064 - categorical_accuracy: 0.5074\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 1.8620 - categorical_accuracy: 0.5369\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 2.9578 - categorical_accuracy: 0.0944\n",
      "model_200_200_10_relu\n",
      "229/229 [==============================] - 0s 920us/step - loss: 3.8009 - categorical_accuracy: 0.0968\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 3.6369 - categorical_accuracy: 0.1021\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 1.5918 - categorical_accuracy: 0.3907\n",
      "229/229 [==============================] - 0s 2ms/step - loss: 1.2183 - categorical_accuracy: 0.7461\n",
      "229/229 [==============================] - 1s 5ms/step - loss: 4.1917 - categorical_accuracy: 0.1277\n",
      "229/229 [==============================] - 2s 7ms/step - loss: 3.8768 - categorical_accuracy: 0.1398\n",
      "229/229 [==============================] - 2s 7ms/step - loss: 3.3141 - categorical_accuracy: 0.1429\n",
      "229/229 [==============================] - 2s 8ms/step - loss: 1.0697 - categorical_accuracy: 0.6461\n",
      "229/229 [==============================] - 0s 2ms/step - loss: 22.3473 - categorical_accuracy: 0.0000e+00\n",
      "229/229 [==============================] - 0s 2ms/step - loss: 5.1176 - categorical_accuracy: 0.1435\n",
      "229/229 [==============================] - 0s 2ms/step - loss: 3.7453 - categorical_accuracy: 0.1444: 0s - loss: 3.7575 - categorical_accura\n",
      "229/229 [==============================] - 0s 2ms/step - loss: 1.1470 - categorical_accuracy: 0.7348\n",
      "229/229 [==============================] - 0s 2ms/step - loss: 6.3980 - categorical_accuracy: 0.1351\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 6.9037 - categorical_accuracy: 0.1233\n",
      "229/229 [==============================] - 0s 2ms/step - loss: 4.2169 - categorical_accuracy: 0.1217\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 1.3290 - categorical_accuracy: 0.7244\n",
      "model_200_200_10_relu_l2_3\n",
      "238/238 [==============================] - 0s 782us/step - loss: 3.7535 - categorical_accuracy: 0.1145\n",
      "238/238 [==============================] - 0s 978us/step - loss: 3.5368 - categorical_accuracy: 0.1200\n",
      "238/238 [==============================] - 0s 803us/step - loss: 1.5291 - categorical_accuracy: 0.4017\n",
      "238/238 [==============================] - 0s 784us/step - loss: 1.1906 - categorical_accuracy: 0.7654\n",
      "238/238 [==============================] - 1s 4ms/step - loss: 3.9578 - categorical_accuracy: 0.1235\n",
      "238/238 [==============================] - 1s 6ms/step - loss: 3.7326 - categorical_accuracy: 0.1530\n",
      "238/238 [==============================] - 1s 5ms/step - loss: 3.1204 - categorical_accuracy: 0.1551\n",
      "238/238 [==============================] - 1s 5ms/step - loss: 1.0088 - categorical_accuracy: 0.6762\n",
      "238/238 [==============================] - 0s 1ms/step - loss: 4.9055 - categorical_accuracy: 0.1431\n",
      "238/238 [==============================] - 0s 1ms/step - loss: 21.8589 - categorical_accuracy: 0.0000e+00\n",
      "238/238 [==============================] - 0s 2ms/step - loss: 3.6202 - categorical_accuracy: 0.1450\n",
      "238/238 [==============================] - 0s 2ms/step - loss: 1.1074 - categorical_accuracy: 0.7604\n",
      "238/238 [==============================] - 0s 1ms/step - loss: 5.7695 - categorical_accuracy: 0.1477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 0s 1ms/step - loss: 6.7633 - categorical_accuracy: 0.1122\n",
      "238/238 [==============================] - 0s 1ms/step - loss: 3.8805 - categorical_accuracy: 0.1433\n",
      "238/238 [==============================] - 0s 2ms/step - loss: 1.2885 - categorical_accuracy: 0.7431\n",
      "model_200_200_10_relu_l2_2\n",
      "273/273 [==============================] - 0s 986us/step - loss: 7.2051 - categorical_accuracy: 0.0230\n",
      "273/273 [==============================] - 0s 921us/step - loss: 6.8183 - categorical_accuracy: 0.0241\n",
      "273/273 [==============================] - 0s 896us/step - loss: 2.9031 - categorical_accuracy: 0.1501\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 1.3127 - categorical_accuracy: 0.6860\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 10.0946 - categorical_accuracy: 0.0149\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 9.5592 - categorical_accuracy: 0.0166\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 8.0216 - categorical_accuracy: 0.0177\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 1.9292 - categorical_accuracy: 0.3020\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 12.6447 - categorical_accuracy: 0.0180\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 12.4918 - categorical_accuracy: 0.0177\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 19.0890 - categorical_accuracy: 0.0000e+00\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 1.4339 - categorical_accuracy: 0.5644\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 14.2495 - categorical_accuracy: 0.0169\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 15.1153 - categorical_accuracy: 0.0206\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 9.7801 - categorical_accuracy: 0.0175\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 1.5382 - categorical_accuracy: 0.6003\n",
      "model_200_200_10_relu_l2_1\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2.4759 - categorical_accuracy: 0.3094\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 2.4823 - categorical_accuracy: 0.2990\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4048 - categorical_accuracy: 0.2212\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3399 - categorical_accuracy: 0.1181\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.9236 - categorical_accuracy: 0.7728\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.9520 - categorical_accuracy: 0.7743\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.8506 - categorical_accuracy: 0.7803\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1.5206 - categorical_accuracy: 0.5336\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.1258 - categorical_accuracy: 0.7758\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.0091 - categorical_accuracy: 0.7683\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.8952 - categorical_accuracy: 0.7788\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 2.3544 - categorical_accuracy: 0.0000e+00\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.1179 - categorical_accuracy: 0.7459\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.2922 - categorical_accuracy: 0.7444\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.9891 - categorical_accuracy: 0.7549\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 2.6168 - categorical_accuracy: 0.0747\n",
      "model_100_100_10_relu\n",
      "216/216 [==============================] - 0s 1ms/step - loss: 3.1149 - categorical_accuracy: 0.1590\n",
      "216/216 [==============================] - 0s 1ms/step - loss: 2.8715 - categorical_accuracy: 0.1793\n",
      "216/216 [==============================] - 0s 1ms/step - loss: 1.3427 - categorical_accuracy: 0.5063\n",
      "216/216 [==============================] - 0s 1ms/step - loss: 1.1947 - categorical_accuracy: 0.7626\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 2.7393 - categorical_accuracy: 0.2490\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 2.4824 - categorical_accuracy: 0.2850\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 2.1952 - categorical_accuracy: 0.2734\n",
      "216/216 [==============================] - 2s 7ms/step - loss: 0.9207 - categorical_accuracy: 0.7372\n",
      "216/216 [==============================] - 0s 2ms/step - loss: 3.8111 - categorical_accuracy: 0.2132\n",
      "216/216 [==============================] - 0s 2ms/step - loss: 3.5150 - categorical_accuracy: 0.2371\n",
      "216/216 [==============================] - 0s 1ms/step - loss: 2.6266 - categorical_accuracy: 0.2531\n",
      "216/216 [==============================] - 1s 2ms/step - loss: 1.0846 - categorical_accuracy: 0.7761\n",
      "216/216 [==============================] - 0s 2ms/step - loss: 22.1884 - categorical_accuracy: 0.0000e+00\n",
      "216/216 [==============================] - 0s 2ms/step - loss: 4.9140 - categorical_accuracy: 0.1994\n",
      "216/216 [==============================] - 0s 2ms/step - loss: 2.9145 - categorical_accuracy: 0.2126\n",
      "216/216 [==============================] - 0s 1ms/step - loss: 1.2785 - categorical_accuracy: 0.7530\n",
      "model_100_100_10_relu_l2_3\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 3.1876 - categorical_accuracy: 0.1598\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 2.9533 - categorical_accuracy: 0.1774\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 1.3431 - categorical_accuracy: 0.4947\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 1.1907 - categorical_accuracy: 0.7608\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2.6036 - categorical_accuracy: 0.2699\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2.4490 - categorical_accuracy: 0.3031\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2.1198 - categorical_accuracy: 0.2956\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 0.8970 - categorical_accuracy: 0.7451\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 3.4573 - categorical_accuracy: 0.2501\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 3.6033 - categorical_accuracy: 0.2416\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 2.4034 - categorical_accuracy: 0.2927\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 1.0734 - categorical_accuracy: 0.7792\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 4.1452 - categorical_accuracy: 0.2580\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 22.9515 - categorical_accuracy: 0.0000e+00\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 2.7671 - categorical_accuracy: 0.2522\n",
      "211/211 [==============================] - 1s 3ms/step - loss: 1.2708 - categorical_accuracy: 0.7558\n",
      "model_100_100_10_relu_l2_2\n",
      "280/280 [==============================] - 0s 945us/step - loss: 7.0906 - categorical_accuracy: 0.0315\n",
      "280/280 [==============================] - 0s 983us/step - loss: 6.6276 - categorical_accuracy: 0.0340\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 2.8159 - categorical_accuracy: 0.1634\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 1.2737 - categorical_accuracy: 0.7084\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 8.8104 - categorical_accuracy: 0.0319\n",
      "280/280 [==============================] - 2s 7ms/step - loss: 8.5033 - categorical_accuracy: 0.0381\n",
      "280/280 [==============================] - 2s 6ms/step - loss: 7.3162 - categorical_accuracy: 0.0329\n",
      "280/280 [==============================] - 2s 7ms/step - loss: 1.7751 - categorical_accuracy: 0.3362\n",
      "280/280 [==============================] - 0s 1ms/step - loss: 11.6445 - categorical_accuracy: 0.0299\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 10.8195 - categorical_accuracy: 0.0405\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 8.3006 - categorical_accuracy: 0.0372\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 1.3716 - categorical_accuracy: 0.6093\n",
      "280/280 [==============================] - 0s 1ms/step - loss: 12.3526 - categorical_accuracy: 0.0367\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 13.6585 - categorical_accuracy: 0.0396\n",
      "280/280 [==============================] - 0s 1ms/step - loss: 18.4450 - categorical_accuracy: 0.0000e+00\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 1.4877 - categorical_accuracy: 0.6182\n",
      "model_100_100_10_relu_l2_1\n",
      "27/27 [==============================] - 0s 933us/step - loss: 2.0734 - categorical_accuracy: 0.3986\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.0751 - categorical_accuracy: 0.3812\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 2.0554 - categorical_accuracy: 0.3140\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 2.2197 - categorical_accuracy: 0.1448\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.7093 - categorical_accuracy: 0.8308\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.7263 - categorical_accuracy: 0.8331\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.6650 - categorical_accuracy: 0.8378\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 1.2719 - categorical_accuracy: 0.6477\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.8719 - categorical_accuracy: 0.8181\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.7833 - categorical_accuracy: 0.8204\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.7020 - categorical_accuracy: 0.8320\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.0545 - categorical_accuracy: 0.2781\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.8800 - categorical_accuracy: 0.7995\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.9981 - categorical_accuracy: 0.8019\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.7708 - categorical_accuracy: 0.8123\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 2.4475 - categorical_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "multiplier = 3\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['names'] = model_names\n",
    "distortion_list = []\n",
    "for i in range(16):\n",
    "    df[model_names[i]] = 0.\n",
    "for i in range(16):\n",
    "    print(model_names[i])\n",
    "    x_perturbed, y_perturbed, distortion = get_perturbations(i, multiplier)\n",
    "    distortion_list.append(distortion)\n",
    "    for j in range(16):\n",
    "        lst = models[j].evaluate(x_perturbed, y_perturbed)\n",
    "        df.iloc[i, j+1] = lst[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>model_10</th>\n",
       "      <th>model_10_l2_3</th>\n",
       "      <th>model_10_l2_2</th>\n",
       "      <th>model_10_l2_1</th>\n",
       "      <th>model_1200_1200_10_relu</th>\n",
       "      <th>model_1200_1200_10_relu_l2_3</th>\n",
       "      <th>model_1200_1200_10_relu_l2_2</th>\n",
       "      <th>model_1200_1200_10_relu_l2_1</th>\n",
       "      <th>model_200_200_10_relu</th>\n",
       "      <th>model_200_200_10_relu_l2_3</th>\n",
       "      <th>model_200_200_10_relu_l2_2</th>\n",
       "      <th>model_200_200_10_relu_l2_1</th>\n",
       "      <th>model_100_100_10_relu</th>\n",
       "      <th>model_100_100_10_relu_l2_3</th>\n",
       "      <th>model_100_100_10_relu_l2_2</th>\n",
       "      <th>model_100_100_10_relu_l2_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>model_10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011496</td>\n",
       "      <td>0.802539</td>\n",
       "      <td>0.456832</td>\n",
       "      <td>0.477428</td>\n",
       "      <td>0.485810</td>\n",
       "      <td>0.700036</td>\n",
       "      <td>0.489522</td>\n",
       "      <td>0.448689</td>\n",
       "      <td>0.507245</td>\n",
       "      <td>0.786852</td>\n",
       "      <td>0.467728</td>\n",
       "      <td>0.374805</td>\n",
       "      <td>0.418273</td>\n",
       "      <td>0.768171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>model_10_l2_3</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010895</td>\n",
       "      <td>0.787808</td>\n",
       "      <td>0.419066</td>\n",
       "      <td>0.431128</td>\n",
       "      <td>0.436706</td>\n",
       "      <td>0.635798</td>\n",
       "      <td>0.431388</td>\n",
       "      <td>0.404799</td>\n",
       "      <td>0.468093</td>\n",
       "      <td>0.748249</td>\n",
       "      <td>0.430610</td>\n",
       "      <td>0.337484</td>\n",
       "      <td>0.375357</td>\n",
       "      <td>0.727367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>model_10_l2_2</td>\n",
       "      <td>0.108808</td>\n",
       "      <td>0.097150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146373</td>\n",
       "      <td>0.797927</td>\n",
       "      <td>0.794041</td>\n",
       "      <td>0.803109</td>\n",
       "      <td>0.589378</td>\n",
       "      <td>0.795337</td>\n",
       "      <td>0.786269</td>\n",
       "      <td>0.800518</td>\n",
       "      <td>0.306995</td>\n",
       "      <td>0.773316</td>\n",
       "      <td>0.766839</td>\n",
       "      <td>0.778497</td>\n",
       "      <td>0.213731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>model_10_l2_1</td>\n",
       "      <td>0.499552</td>\n",
       "      <td>0.488809</td>\n",
       "      <td>0.430618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.865712</td>\n",
       "      <td>0.863026</td>\n",
       "      <td>0.868397</td>\n",
       "      <td>0.727842</td>\n",
       "      <td>0.855864</td>\n",
       "      <td>0.853178</td>\n",
       "      <td>0.861235</td>\n",
       "      <td>0.472695</td>\n",
       "      <td>0.845121</td>\n",
       "      <td>0.845121</td>\n",
       "      <td>0.849597</td>\n",
       "      <td>0.338406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>model_1200_1200_10_relu</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.026794</td>\n",
       "      <td>0.121484</td>\n",
       "      <td>0.688288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014549</td>\n",
       "      <td>0.016368</td>\n",
       "      <td>0.327837</td>\n",
       "      <td>0.019520</td>\n",
       "      <td>0.022551</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>0.600024</td>\n",
       "      <td>0.024612</td>\n",
       "      <td>0.022672</td>\n",
       "      <td>0.024733</td>\n",
       "      <td>0.621605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>model_1200_1200_10_relu_l2_3</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>0.026431</td>\n",
       "      <td>0.119908</td>\n",
       "      <td>0.684530</td>\n",
       "      <td>0.010184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011518</td>\n",
       "      <td>0.308196</td>\n",
       "      <td>0.017095</td>\n",
       "      <td>0.017944</td>\n",
       "      <td>0.022793</td>\n",
       "      <td>0.581959</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>0.019762</td>\n",
       "      <td>0.022551</td>\n",
       "      <td>0.612997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>model_1200_1200_10_relu_l2_2</td>\n",
       "      <td>0.010678</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>0.047032</td>\n",
       "      <td>0.592348</td>\n",
       "      <td>0.005339</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080336</td>\n",
       "      <td>0.008008</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>0.370662</td>\n",
       "      <td>0.008644</td>\n",
       "      <td>0.007754</td>\n",
       "      <td>0.008644</td>\n",
       "      <td>0.428626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>model_1200_1200_10_relu_l2_1</td>\n",
       "      <td>0.123894</td>\n",
       "      <td>0.112094</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>0.091445</td>\n",
       "      <td>0.539823</td>\n",
       "      <td>0.548673</td>\n",
       "      <td>0.563422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533923</td>\n",
       "      <td>0.542773</td>\n",
       "      <td>0.542773</td>\n",
       "      <td>0.079646</td>\n",
       "      <td>0.519174</td>\n",
       "      <td>0.507375</td>\n",
       "      <td>0.536873</td>\n",
       "      <td>0.094395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>model_200_200_10_relu</td>\n",
       "      <td>0.096787</td>\n",
       "      <td>0.102119</td>\n",
       "      <td>0.390704</td>\n",
       "      <td>0.746138</td>\n",
       "      <td>0.127683</td>\n",
       "      <td>0.139850</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.646070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143541</td>\n",
       "      <td>0.144361</td>\n",
       "      <td>0.734792</td>\n",
       "      <td>0.135065</td>\n",
       "      <td>0.123308</td>\n",
       "      <td>0.121668</td>\n",
       "      <td>0.724402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>model_200_200_10_relu_l2_3</td>\n",
       "      <td>0.114459</td>\n",
       "      <td>0.119984</td>\n",
       "      <td>0.401658</td>\n",
       "      <td>0.765426</td>\n",
       "      <td>0.123536</td>\n",
       "      <td>0.153006</td>\n",
       "      <td>0.155111</td>\n",
       "      <td>0.676227</td>\n",
       "      <td>0.143139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144981</td>\n",
       "      <td>0.760426</td>\n",
       "      <td>0.147744</td>\n",
       "      <td>0.112222</td>\n",
       "      <td>0.143271</td>\n",
       "      <td>0.743060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>model_200_200_10_relu_l2_2</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.024080</td>\n",
       "      <td>0.150097</td>\n",
       "      <td>0.686045</td>\n",
       "      <td>0.014907</td>\n",
       "      <td>0.016627</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.302030</td>\n",
       "      <td>0.018003</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564385</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.600275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>model_200_200_10_relu_l2_1</td>\n",
       "      <td>0.309417</td>\n",
       "      <td>0.298954</td>\n",
       "      <td>0.221226</td>\n",
       "      <td>0.118087</td>\n",
       "      <td>0.772795</td>\n",
       "      <td>0.774290</td>\n",
       "      <td>0.780269</td>\n",
       "      <td>0.533632</td>\n",
       "      <td>0.775785</td>\n",
       "      <td>0.768311</td>\n",
       "      <td>0.778774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.745889</td>\n",
       "      <td>0.744395</td>\n",
       "      <td>0.754858</td>\n",
       "      <td>0.074738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>model_100_100_10_relu</td>\n",
       "      <td>0.158956</td>\n",
       "      <td>0.179260</td>\n",
       "      <td>0.506309</td>\n",
       "      <td>0.762582</td>\n",
       "      <td>0.249021</td>\n",
       "      <td>0.284989</td>\n",
       "      <td>0.273387</td>\n",
       "      <td>0.737201</td>\n",
       "      <td>0.213198</td>\n",
       "      <td>0.237128</td>\n",
       "      <td>0.253082</td>\n",
       "      <td>0.776070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199420</td>\n",
       "      <td>0.212618</td>\n",
       "      <td>0.753009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>model_100_100_10_relu_l2_3</td>\n",
       "      <td>0.159751</td>\n",
       "      <td>0.177386</td>\n",
       "      <td>0.494665</td>\n",
       "      <td>0.760818</td>\n",
       "      <td>0.269858</td>\n",
       "      <td>0.303053</td>\n",
       "      <td>0.295643</td>\n",
       "      <td>0.745110</td>\n",
       "      <td>0.250148</td>\n",
       "      <td>0.241553</td>\n",
       "      <td>0.292679</td>\n",
       "      <td>0.779194</td>\n",
       "      <td>0.258002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252223</td>\n",
       "      <td>0.755780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>model_100_100_10_relu_l2_2</td>\n",
       "      <td>0.031470</td>\n",
       "      <td>0.034046</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.708366</td>\n",
       "      <td>0.031918</td>\n",
       "      <td>0.038078</td>\n",
       "      <td>0.032926</td>\n",
       "      <td>0.336208</td>\n",
       "      <td>0.029903</td>\n",
       "      <td>0.040542</td>\n",
       "      <td>0.037182</td>\n",
       "      <td>0.609251</td>\n",
       "      <td>0.036734</td>\n",
       "      <td>0.039646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.618210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>model_100_100_10_relu_l2_1</td>\n",
       "      <td>0.398609</td>\n",
       "      <td>0.381228</td>\n",
       "      <td>0.314021</td>\n",
       "      <td>0.144844</td>\n",
       "      <td>0.830823</td>\n",
       "      <td>0.833140</td>\n",
       "      <td>0.837775</td>\n",
       "      <td>0.647740</td>\n",
       "      <td>0.818076</td>\n",
       "      <td>0.820394</td>\n",
       "      <td>0.831981</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.799537</td>\n",
       "      <td>0.801854</td>\n",
       "      <td>0.812283</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           names  model_10  model_10_l2_3  model_10_l2_2  \\\n",
       "0                       model_10  0.000000       0.000000       0.011496   \n",
       "1                  model_10_l2_3  0.000130       0.000000       0.010895   \n",
       "2                  model_10_l2_2  0.108808       0.097150       0.000000   \n",
       "3                  model_10_l2_1  0.499552       0.488809       0.430618   \n",
       "4        model_1200_1200_10_relu  0.023400       0.026794       0.121484   \n",
       "5   model_1200_1200_10_relu_l2_3  0.024127       0.026431       0.119908   \n",
       "6   model_1200_1200_10_relu_l2_2  0.010678       0.011440       0.047032   \n",
       "7   model_1200_1200_10_relu_l2_1  0.123894       0.112094       0.088496   \n",
       "8          model_200_200_10_relu  0.096787       0.102119       0.390704   \n",
       "9     model_200_200_10_relu_l2_3  0.114459       0.119984       0.401658   \n",
       "10    model_200_200_10_relu_l2_2  0.023048       0.024080       0.150097   \n",
       "11    model_200_200_10_relu_l2_1  0.309417       0.298954       0.221226   \n",
       "12         model_100_100_10_relu  0.158956       0.179260       0.506309   \n",
       "13    model_100_100_10_relu_l2_3  0.159751       0.177386       0.494665   \n",
       "14    model_100_100_10_relu_l2_2  0.031470       0.034046       0.163400   \n",
       "15    model_100_100_10_relu_l2_1  0.398609       0.381228       0.314021   \n",
       "\n",
       "    model_10_l2_1  model_1200_1200_10_relu  model_1200_1200_10_relu_l2_3  \\\n",
       "0        0.802539                 0.456832                      0.477428   \n",
       "1        0.787808                 0.419066                      0.431128   \n",
       "2        0.146373                 0.797927                      0.794041   \n",
       "3        0.000000                 0.865712                      0.863026   \n",
       "4        0.688288                 0.000000                      0.014549   \n",
       "5        0.684530                 0.010184                      0.000000   \n",
       "6        0.592348                 0.005339                      0.004576   \n",
       "7        0.091445                 0.539823                      0.548673   \n",
       "8        0.746138                 0.127683                      0.139850   \n",
       "9        0.765426                 0.123536                      0.153006   \n",
       "10       0.686045                 0.014907                      0.016627   \n",
       "11       0.118087                 0.772795                      0.774290   \n",
       "12       0.762582                 0.249021                      0.284989   \n",
       "13       0.760818                 0.269858                      0.303053   \n",
       "14       0.708366                 0.031918                      0.038078   \n",
       "15       0.144844                 0.830823                      0.833140   \n",
       "\n",
       "    model_1200_1200_10_relu_l2_2  model_1200_1200_10_relu_l2_1  \\\n",
       "0                       0.485810                      0.700036   \n",
       "1                       0.436706                      0.635798   \n",
       "2                       0.803109                      0.589378   \n",
       "3                       0.868397                      0.727842   \n",
       "4                       0.016368                      0.327837   \n",
       "5                       0.011518                      0.308196   \n",
       "6                       0.000000                      0.080336   \n",
       "7                       0.563422                      0.000000   \n",
       "8                       0.142857                      0.646070   \n",
       "9                       0.155111                      0.676227   \n",
       "10                      0.017659                      0.302030   \n",
       "11                      0.780269                      0.533632   \n",
       "12                      0.273387                      0.737201   \n",
       "13                      0.295643                      0.745110   \n",
       "14                      0.032926                      0.336208   \n",
       "15                      0.837775                      0.647740   \n",
       "\n",
       "    model_200_200_10_relu  model_200_200_10_relu_l2_3  \\\n",
       "0                0.489522                    0.448689   \n",
       "1                0.431388                    0.404799   \n",
       "2                0.795337                    0.786269   \n",
       "3                0.855864                    0.853178   \n",
       "4                0.019520                    0.022551   \n",
       "5                0.017095                    0.017944   \n",
       "6                0.008008                    0.007500   \n",
       "7                0.533923                    0.542773   \n",
       "8                0.000000                    0.143541   \n",
       "9                0.143139                    0.000000   \n",
       "10               0.018003                    0.017659   \n",
       "11               0.775785                    0.768311   \n",
       "12               0.213198                    0.237128   \n",
       "13               0.250148                    0.241553   \n",
       "14               0.029903                    0.040542   \n",
       "15               0.818076                    0.820394   \n",
       "\n",
       "    model_200_200_10_relu_l2_2  model_200_200_10_relu_l2_1  \\\n",
       "0                     0.507245                    0.786852   \n",
       "1                     0.468093                    0.748249   \n",
       "2                     0.800518                    0.306995   \n",
       "3                     0.861235                    0.472695   \n",
       "4                     0.024127                    0.600024   \n",
       "5                     0.022793                    0.581959   \n",
       "6                     0.008262                    0.370662   \n",
       "7                     0.542773                    0.079646   \n",
       "8                     0.144361                    0.734792   \n",
       "9                     0.144981                    0.760426   \n",
       "10                    0.000000                    0.564385   \n",
       "11                    0.778774                    0.000000   \n",
       "12                    0.253082                    0.776070   \n",
       "13                    0.292679                    0.779194   \n",
       "14                    0.037182                    0.609251   \n",
       "15                    0.831981                    0.278100   \n",
       "\n",
       "    model_100_100_10_relu  model_100_100_10_relu_l2_3  \\\n",
       "0                0.467728                    0.374805   \n",
       "1                0.430610                    0.337484   \n",
       "2                0.773316                    0.766839   \n",
       "3                0.845121                    0.845121   \n",
       "4                0.024612                    0.022672   \n",
       "5                0.020369                    0.019762   \n",
       "6                0.008644                    0.007754   \n",
       "7                0.519174                    0.507375   \n",
       "8                0.135065                    0.123308   \n",
       "9                0.147744                    0.112222   \n",
       "10               0.016856                    0.020640   \n",
       "11               0.745889                    0.744395   \n",
       "12               0.000000                    0.199420   \n",
       "13               0.258002                    0.000000   \n",
       "14               0.036734                    0.039646   \n",
       "15               0.799537                    0.801854   \n",
       "\n",
       "    model_100_100_10_relu_l2_2  model_100_100_10_relu_l2_1  \n",
       "0                     0.418273                    0.768171  \n",
       "1                     0.375357                    0.727367  \n",
       "2                     0.778497                    0.213731  \n",
       "3                     0.849597                    0.338406  \n",
       "4                     0.024733                    0.621605  \n",
       "5                     0.022551                    0.612997  \n",
       "6                     0.008644                    0.428626  \n",
       "7                     0.536873                    0.094395  \n",
       "8                     0.121668                    0.724402  \n",
       "9                     0.143271                    0.743060  \n",
       "10                    0.017544                    0.600275  \n",
       "11                    0.754858                    0.074738  \n",
       "12                    0.212618                    0.753009  \n",
       "13                    0.252223                    0.755780  \n",
       "14                    0.000000                    0.618210  \n",
       "15                    0.812283                    0.000000  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "# df.to_csv('models_r_3_0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmRbdyiyTT7J"
   },
   "source": [
    "Run Bayesian NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dllHvJyhhNPL",
    "outputId": "e783bd67-a76f-4da1-abf8-78078857aa4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8438871340453624\n"
     ]
    }
   ],
   "source": [
    "MODEL_BAYESIAN = bayes_models[0]\n",
    "\n",
    "eval_samples_list = [1]\n",
    "\n",
    "df_bayes = pd.DataFrame()\n",
    "df_bayes['names'] = model_names\n",
    "\n",
    "for j in range(len(eval_samples_list)):\n",
    "    eval_samples = eval_samples_list[j]\n",
    "    df_bayes['accuracy with '+str(eval_samples)] = [0. for i in range(16)]\n",
    "    \n",
    "    for i in range(16):\n",
    "        x_perturbed, y_perturbed, distortion = get_perturbations(i, 1.5)\n",
    "\n",
    "        acc = evaluate_bayesian(x_perturbed, y_perturbed, eval_samples)\n",
    "\n",
    "        df_bayes.iloc[i,j+1] = float(acc)\n",
    "    print(np.mean(df_bayes.iloc[:,j+1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fq-4-MDlhNUZ",
    "outputId": "46596f0f-fc6e-4be4-c73d-596c6b88ee4c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>accuracy with 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>model_10</td>\n",
       "      <td>0.951397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>model_10_l2_3</td>\n",
       "      <td>0.946811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>model_10_l2_2</td>\n",
       "      <td>0.774391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>model_10_l2_1</td>\n",
       "      <td>0.834843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>model_100_100_10_relu</td>\n",
       "      <td>0.921983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>model_100_100_10_relu_l2_3</td>\n",
       "      <td>0.925279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>model_100_100_10_relu_l2_2</td>\n",
       "      <td>0.888622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>model_100_100_10_relu_l2_1</td>\n",
       "      <td>0.801564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>model_200_200_10_relu</td>\n",
       "      <td>0.914085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>model_200_200_10_relu_l2_3</td>\n",
       "      <td>0.921538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>model_200_200_10_relu_l2_2</td>\n",
       "      <td>0.864306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>model_200_200_10_relu_l2_1</td>\n",
       "      <td>0.743901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>model_1200_1200_10_relu</td>\n",
       "      <td>0.865702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>model_1200_1200_10_relu_l2_3</td>\n",
       "      <td>0.858214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>model_1200_1200_10_relu_l2_2</td>\n",
       "      <td>0.751977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>model_1200_1200_10_relu_l2_1</td>\n",
       "      <td>0.537581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           names  accuracy with 1\n",
       "0                       model_10         0.951397\n",
       "1                  model_10_l2_3         0.946811\n",
       "2                  model_10_l2_2         0.774391\n",
       "3                  model_10_l2_1         0.834843\n",
       "4          model_100_100_10_relu         0.921983\n",
       "5     model_100_100_10_relu_l2_3         0.925279\n",
       "6     model_100_100_10_relu_l2_2         0.888622\n",
       "7     model_100_100_10_relu_l2_1         0.801564\n",
       "8          model_200_200_10_relu         0.914085\n",
       "9     model_200_200_10_relu_l2_3         0.921538\n",
       "10    model_200_200_10_relu_l2_2         0.864306\n",
       "11    model_200_200_10_relu_l2_1         0.743901\n",
       "12       model_1200_1200_10_relu         0.865702\n",
       "13  model_1200_1200_10_relu_l2_3         0.858214\n",
       "14  model_1200_1200_10_relu_l2_2         0.751977\n",
       "15  model_1200_1200_10_relu_l2_1         0.537581"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "VnrFZjJzfnTE"
   },
   "outputs": [],
   "source": [
    "df_bayes.to_csv('bayes_4_0_r_1_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQIk_xdyfnbd"
   },
   "source": [
    "## Accuracy under Perturbation and Model Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "1QwdOeyZfneZ"
   },
   "outputs": [],
   "source": [
    "def prune(w, remove_ratio):\n",
    "    '''\n",
    "    Weights pruning.\n",
    "    This function depends on the knowledge that, model weights follows this format:\n",
    "    for each layer:\n",
    "        weight_mu\n",
    "        bias_mu\n",
    "        weight_rho\n",
    "        bias_rho\n",
    "    '''\n",
    "\n",
    "    sn_ratio = []\n",
    "    for i in range(0, len(w), 4):\n",
    "        sn_ratio.append(w[i]/tf.math.softplus(w[i+2]))\n",
    "        sn_ratio.append(w[i+1]/tf.math.softplus(w[i+3]))\n",
    "\n",
    "    sorted_ratio = []\n",
    "    for i in range(len(sn_ratio)):\n",
    "        sorted_ratio.append(np.argsort(np.reshape(abs(sn_ratio[i]), (-1,))))\n",
    "\n",
    "    new_weights = deepcopy(w)\n",
    "    for i in range(0, len(sorted_ratio), 2):\n",
    "        remove_until_1 = min(int(len(sorted_ratio[i]) * remove_ratio), (len(sorted_ratio[i])-1))\n",
    "        remove_until_2 = min(int(len(sorted_ratio[i+1]) * remove_ratio), (len(sorted_ratio[i+1])-1))\n",
    "        for j in range(remove_until_1):\n",
    "            rmv_idx = sorted_ratio[i][j]\n",
    "            idx1 = int(rmv_idx / w[i*2].shape[1])\n",
    "            idx2 = rmv_idx - idx1 * w[i*2].shape[1]\n",
    "            new_weights[i*2][idx1, idx2] = 0.0\n",
    "            new_weights[i*2+2][idx1, idx2] = -100.0\n",
    "        for j in range(remove_until_2):\n",
    "            idx = sorted_ratio[i+1][j]\n",
    "            new_weights[i*2+1][idx] = 0.0\n",
    "            new_weights[i*2+3][idx] = -100.0\n",
    "            \n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bayes_2_0\n",
      "0.48967552185058594\n",
      "0.48672565817832947\n",
      "0.4601770043373108\n",
      "0.4336283206939697\n",
      "0.3687315583229065\n",
      "0.2507374584674835\n"
     ]
    }
   ],
   "source": [
    "print(bayes_names[5])\n",
    "MODEL_BAYESIAN = bayes_models[5] # rho initialized to -2\n",
    "eval_samples = 100\n",
    "\n",
    "w = MODEL_BAYESIAN.get_weights()\n",
    "w_archive = deepcopy(w)\n",
    "\n",
    "df_prune = pd.DataFrame()\n",
    "df_prune['names'] = model_names\n",
    "ratios = [0.25,0.5,0.75,0.9,0.95,0.98]\n",
    "\n",
    "for j in range(len(ratios)):\n",
    "\n",
    "    w = MODEL_BAYESIAN.get_weights()\n",
    "    ratio = ratios[j]\n",
    "    new_w = prune(w, ratio)\n",
    "    MODEL_BAYESIAN.set_weights(new_w)\n",
    "\n",
    "    df_prune[str(ratio)] = [0. for i in range(16)]\n",
    "\n",
    "    for i in range(16):\n",
    "        x_perturbed, y_perturbed, distortion = get_perturbations(i, 1.5)\n",
    "\n",
    "        acc = evaluate_bayesian(x_perturbed, y_perturbed, eval_samples)\n",
    "\n",
    "        df_prune.iloc[i,j+1] = float(acc)\n",
    "\n",
    "    # acc = evaluate_bayesian(x_test, y_test, eval_samples)\n",
    "\n",
    "    MODEL_BAYESIAN.set_weights(w_archive)\n",
    "    \n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.75</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.95</th>\n",
       "      <th>0.98</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>model_10</td>\n",
       "      <td>0.966351</td>\n",
       "      <td>0.963956</td>\n",
       "      <td>0.962040</td>\n",
       "      <td>0.941803</td>\n",
       "      <td>0.819064</td>\n",
       "      <td>0.538977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>model_10_l2_3</td>\n",
       "      <td>0.960700</td>\n",
       "      <td>0.961219</td>\n",
       "      <td>0.958236</td>\n",
       "      <td>0.938392</td>\n",
       "      <td>0.810376</td>\n",
       "      <td>0.535019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>model_10_l2_2</td>\n",
       "      <td>0.759067</td>\n",
       "      <td>0.757772</td>\n",
       "      <td>0.753886</td>\n",
       "      <td>0.713731</td>\n",
       "      <td>0.568653</td>\n",
       "      <td>0.326425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>model_10_l2_1</td>\n",
       "      <td>0.829006</td>\n",
       "      <td>0.831692</td>\n",
       "      <td>0.824530</td>\n",
       "      <td>0.784244</td>\n",
       "      <td>0.638317</td>\n",
       "      <td>0.359893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>model_100_100_10_relu</td>\n",
       "      <td>0.946483</td>\n",
       "      <td>0.945468</td>\n",
       "      <td>0.939376</td>\n",
       "      <td>0.908920</td>\n",
       "      <td>0.781871</td>\n",
       "      <td>0.517041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>model_100_100_10_relu_l2_3</td>\n",
       "      <td>0.949022</td>\n",
       "      <td>0.945762</td>\n",
       "      <td>0.943391</td>\n",
       "      <td>0.904861</td>\n",
       "      <td>0.787937</td>\n",
       "      <td>0.538826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>model_100_100_10_relu_l2_2</td>\n",
       "      <td>0.927315</td>\n",
       "      <td>0.924068</td>\n",
       "      <td>0.909620</td>\n",
       "      <td>0.851831</td>\n",
       "      <td>0.688655</td>\n",
       "      <td>0.428491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>model_100_100_10_relu_l2_1</td>\n",
       "      <td>0.789108</td>\n",
       "      <td>0.787949</td>\n",
       "      <td>0.779838</td>\n",
       "      <td>0.743917</td>\n",
       "      <td>0.600232</td>\n",
       "      <td>0.329085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>model_200_200_10_relu</td>\n",
       "      <td>0.948599</td>\n",
       "      <td>0.946685</td>\n",
       "      <td>0.938893</td>\n",
       "      <td>0.896377</td>\n",
       "      <td>0.755161</td>\n",
       "      <td>0.495967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>model_200_200_10_relu_l2_3</td>\n",
       "      <td>0.949612</td>\n",
       "      <td>0.948428</td>\n",
       "      <td>0.938298</td>\n",
       "      <td>0.906065</td>\n",
       "      <td>0.764636</td>\n",
       "      <td>0.502829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>model_200_200_10_relu_l2_2</td>\n",
       "      <td>0.904598</td>\n",
       "      <td>0.907121</td>\n",
       "      <td>0.879486</td>\n",
       "      <td>0.815618</td>\n",
       "      <td>0.655888</td>\n",
       "      <td>0.419677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>model_200_200_10_relu_l2_1</td>\n",
       "      <td>0.730942</td>\n",
       "      <td>0.736921</td>\n",
       "      <td>0.723468</td>\n",
       "      <td>0.681614</td>\n",
       "      <td>0.547085</td>\n",
       "      <td>0.315396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>model_1200_1200_10_relu</td>\n",
       "      <td>0.917192</td>\n",
       "      <td>0.916707</td>\n",
       "      <td>0.898157</td>\n",
       "      <td>0.843477</td>\n",
       "      <td>0.692774</td>\n",
       "      <td>0.445805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>model_1200_1200_10_relu_l2_3</td>\n",
       "      <td>0.916950</td>\n",
       "      <td>0.912342</td>\n",
       "      <td>0.894399</td>\n",
       "      <td>0.836324</td>\n",
       "      <td>0.683196</td>\n",
       "      <td>0.437197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>model_1200_1200_10_relu_l2_2</td>\n",
       "      <td>0.820389</td>\n",
       "      <td>0.819880</td>\n",
       "      <td>0.790136</td>\n",
       "      <td>0.724037</td>\n",
       "      <td>0.591331</td>\n",
       "      <td>0.387695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>model_1200_1200_10_relu_l2_1</td>\n",
       "      <td>0.489676</td>\n",
       "      <td>0.486726</td>\n",
       "      <td>0.460177</td>\n",
       "      <td>0.433628</td>\n",
       "      <td>0.368732</td>\n",
       "      <td>0.250737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           names      0.25       0.5      0.75       0.9  \\\n",
       "0                       model_10  0.966351  0.963956  0.962040  0.941803   \n",
       "1                  model_10_l2_3  0.960700  0.961219  0.958236  0.938392   \n",
       "2                  model_10_l2_2  0.759067  0.757772  0.753886  0.713731   \n",
       "3                  model_10_l2_1  0.829006  0.831692  0.824530  0.784244   \n",
       "4          model_100_100_10_relu  0.946483  0.945468  0.939376  0.908920   \n",
       "5     model_100_100_10_relu_l2_3  0.949022  0.945762  0.943391  0.904861   \n",
       "6     model_100_100_10_relu_l2_2  0.927315  0.924068  0.909620  0.851831   \n",
       "7     model_100_100_10_relu_l2_1  0.789108  0.787949  0.779838  0.743917   \n",
       "8          model_200_200_10_relu  0.948599  0.946685  0.938893  0.896377   \n",
       "9     model_200_200_10_relu_l2_3  0.949612  0.948428  0.938298  0.906065   \n",
       "10    model_200_200_10_relu_l2_2  0.904598  0.907121  0.879486  0.815618   \n",
       "11    model_200_200_10_relu_l2_1  0.730942  0.736921  0.723468  0.681614   \n",
       "12       model_1200_1200_10_relu  0.917192  0.916707  0.898157  0.843477   \n",
       "13  model_1200_1200_10_relu_l2_3  0.916950  0.912342  0.894399  0.836324   \n",
       "14  model_1200_1200_10_relu_l2_2  0.820389  0.819880  0.790136  0.724037   \n",
       "15  model_1200_1200_10_relu_l2_1  0.489676  0.486726  0.460177  0.433628   \n",
       "\n",
       "        0.95      0.98  \n",
       "0   0.819064  0.538977  \n",
       "1   0.810376  0.535019  \n",
       "2   0.568653  0.326425  \n",
       "3   0.638317  0.359893  \n",
       "4   0.781871  0.517041  \n",
       "5   0.787937  0.538826  \n",
       "6   0.688655  0.428491  \n",
       "7   0.600232  0.329085  \n",
       "8   0.755161  0.495967  \n",
       "9   0.764636  0.502829  \n",
       "10  0.655888  0.419677  \n",
       "11  0.547085  0.315396  \n",
       "12  0.692774  0.445805  \n",
       "13  0.683196  0.437197  \n",
       "14  0.591331  0.387695  \n",
       "15  0.368732  0.250737  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prune.to_csv('prune_2_0_r_1_5_s_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEuCp1JvfnkS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gonR1efZjUj"
   },
   "source": [
    "Scratches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hm6GfHcp8Cu2",
    "outputId": "b477649c-9b36-4a29-ab46-98ffcd8946f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -8.09932945 -21.45716771   3.31232257  22.68672388 -43.33224414\n",
      "  50.59693345   4.81355797 -13.91471448  11.31174654  11.68219634]\n"
     ]
    }
   ],
   "source": [
    "weights = np.load(path+'/weights_rho_rnd_2_4.npy', allow_pickle=True).flatten()[0]\n",
    "\n",
    "nn = Neural_network_numpy(weights)\n",
    "\n",
    "predictions = nn.predict(np.reshape(x_train[0], (-1,)))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VXfk6iecWf-"
   },
   "outputs": [],
   "source": [
    "def loss_fn(y_true, logits, from_logits=True):\n",
    "    if from_logits:\n",
    "        logits = np.squeeze(logits)\n",
    "        y_true = np.squeeze(y_true)\n",
    "        denom = sum(np.exp(logits))\n",
    "        y_pred = np.exp(logits)/denom\n",
    "        true_idx = np.where(np.array(y_true)==1)[0][0]\n",
    "        return -np.log(y_pred[true_idx])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vk_9qcxFXk45"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, Bounds\n",
    "\n",
    "def objective(r, model_numpy, x, y):\n",
    "    norm = np.linalg.norm(r)\n",
    "    perturbed_img = np.array([r + np.reshape(x, (-1,))])\n",
    "    logits = model_numpy.predict(perturbed_img)\n",
    "    loss_value = loss_fn(y, logits)\n",
    "    print(r)\n",
    "    return (norm + loss_value)\n",
    "\n",
    "minimize(objective, np.zeros((1,784))+2e-3, args=(nn,x_train[0],y_train[0]), method='Nelder-Mead', options={'maxiter':2, 'maxfev':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DPSjMSbXk7c",
    "outputId": "8921911a-01b2-4735-cf7b-bc77c94ed76d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array([0,0,1])==1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz8Ab9uRXk95"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrjFYjGbXlBe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0KIGbF0XlFj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tBxcTTHPQCR"
   },
   "outputs": [],
   "source": [
    "def f(x, a, b):\n",
    "    if a and b:\n",
    "        return sum(x**2)\n",
    "    else:\n",
    "        return 10\n",
    "\n",
    "minimize(f, np.array([2,2.5]), args=(1,1), method='L-BFGS-B', bounds=[[0.5, 3],[0,3]])\n",
    "\n",
    "#np.random.normal(loc=0.5, scale=0.2, size=(img_shape1,img_shape2))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "perturbation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
